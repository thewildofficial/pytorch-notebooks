{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN1hLzpBZzH9eimKO57ZaZx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thewildofficial/pytorch-notebooks/blob/main/Space_Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "yW5-TxfafqIN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "train_df = pd.read_csv(\"https://raw.githubusercontent.com/pirosbogar/spaceship-titanic/main/train.csv\")\n",
        "test_df = pd.read_csv(\"https://raw.githubusercontent.com/pirosbogar/spaceship-titanic/main/test.csv\")\n",
        "submitted_vals = pd.read_csv(\"https://raw.githubusercontent.com/pirosbogar/spaceship-titanic/main/submission.csv\")\n",
        "\n",
        "test_df[\"Transported\"] = submitted_vals[\"Transported\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "zPmqIz7agl4k",
        "outputId": "99960d22-a582-4cb1-f0e3-ce220de09c32"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
              "0        0013_01      Earth      True     G/3/S    TRAPPIST-1e  27.0  False   \n",
              "1        0018_01      Earth     False     F/4/S    TRAPPIST-1e  19.0  False   \n",
              "2        0019_01     Europa      True     C/0/S    55 Cancri e  31.0  False   \n",
              "3        0021_01     Europa     False     C/1/S    TRAPPIST-1e  38.0  False   \n",
              "4        0023_01      Earth     False     F/5/S    TRAPPIST-1e  20.0  False   \n",
              "...          ...        ...       ...       ...            ...   ...    ...   \n",
              "4272     9266_02      Earth      True  G/1496/S    TRAPPIST-1e  34.0  False   \n",
              "4273     9269_01      Earth     False       NaN    TRAPPIST-1e  42.0  False   \n",
              "4274     9271_01       Mars      True   D/296/P    55 Cancri e   NaN  False   \n",
              "4275     9273_01     Europa     False   D/297/P            NaN   NaN  False   \n",
              "4276     9277_01      Earth      True  G/1498/S  PSO J318.5-22  43.0  False   \n",
              "\n",
              "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck              Name  \\\n",
              "0             0.0        0.0           0.0     0.0     0.0   Nelly Carsoning   \n",
              "1             0.0        9.0           0.0  2823.0     0.0    Lerome Peckers   \n",
              "2             0.0        0.0           0.0     0.0     0.0   Sabih Unhearfus   \n",
              "3             0.0     6652.0           0.0   181.0   585.0  Meratz Caltilter   \n",
              "4            10.0        0.0         635.0     0.0     0.0   Brence Harperez   \n",
              "...           ...        ...           ...     ...     ...               ...   \n",
              "4272          0.0        0.0           0.0     0.0     0.0       Jeron Peter   \n",
              "4273          0.0      847.0          17.0    10.0   144.0     Matty Scheron   \n",
              "4274          0.0        0.0           0.0     0.0     0.0       Jayrin Pore   \n",
              "4275          0.0     2680.0           0.0     0.0   523.0    Kitakan Conale   \n",
              "4276          0.0        0.0           0.0     0.0     0.0  Lilace Leonzaley   \n",
              "\n",
              "      Transported  \n",
              "0            True  \n",
              "1           False  \n",
              "2            True  \n",
              "3            True  \n",
              "4            True  \n",
              "...           ...  \n",
              "4272         True  \n",
              "4273        False  \n",
              "4274         True  \n",
              "4275         True  \n",
              "4276         True  \n",
              "\n",
              "[4277 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-efe26151-bffb-451e-b19d-5a8915b236f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>HomePlanet</th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Name</th>\n",
              "      <th>Transported</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0013_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>True</td>\n",
              "      <td>G/3/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>27.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Nelly Carsoning</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0018_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>F/4/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>19.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2823.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Lerome Peckers</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0019_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>True</td>\n",
              "      <td>C/0/S</td>\n",
              "      <td>55 Cancri e</td>\n",
              "      <td>31.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Sabih Unhearfus</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0021_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>C/1/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>38.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6652.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>585.0</td>\n",
              "      <td>Meratz Caltilter</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0023_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>F/5/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>20.0</td>\n",
              "      <td>False</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>635.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Brence Harperez</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4272</th>\n",
              "      <td>9266_02</td>\n",
              "      <td>Earth</td>\n",
              "      <td>True</td>\n",
              "      <td>G/1496/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>34.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Jeron Peter</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4273</th>\n",
              "      <td>9269_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>42.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>847.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>Matty Scheron</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4274</th>\n",
              "      <td>9271_01</td>\n",
              "      <td>Mars</td>\n",
              "      <td>True</td>\n",
              "      <td>D/296/P</td>\n",
              "      <td>55 Cancri e</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Jayrin Pore</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4275</th>\n",
              "      <td>9273_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>D/297/P</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2680.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>523.0</td>\n",
              "      <td>Kitakan Conale</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4276</th>\n",
              "      <td>9277_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>True</td>\n",
              "      <td>G/1498/S</td>\n",
              "      <td>PSO J318.5-22</td>\n",
              "      <td>43.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Lilace Leonzaley</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4277 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efe26151-bffb-451e-b19d-5a8915b236f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-efe26151-bffb-451e-b19d-5a8915b236f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-efe26151-bffb-451e-b19d-5a8915b236f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "features = [\"HomePlanet\",\"CryoSleep\",\"Age\",\"VIP\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\",\"Transported\"]\n",
        "for i, df in enumerate([train_df, test_df]):\n",
        "  df[\"HomePlanet\"] = df[\"HomePlanet\"].replace({\"Europa\": 0, \"Earth\": 1, \"Mars\": 2})\n",
        "  df = df[features].astype(float).dropna(axis=0)\n",
        "  scaler = MinMaxScaler()\n",
        "  normalize_columns = [\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"]\n",
        "  df[normalize_columns] = scaler.fit_transform(df[normalize_columns])\n",
        "  if i == 0:\n",
        "    train_df = df\n",
        "  else:\n",
        "    test_df = df"
      ],
      "metadata": {
        "id": "KlihATOugoRz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "70ADvvXohmN6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "f6fa1c4b-13d6-4a89-a310-846ecdc219aa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      HomePlanet  CryoSleep   Age  VIP  RoomService  FoodCourt  ShoppingMall  \\\n",
              "0            0.0        0.0  39.0  0.0     0.000000   0.000000      0.000000   \n",
              "1            1.0        0.0  24.0  0.0     0.007608   0.000302      0.001064   \n",
              "2            0.0        0.0  58.0  1.0     0.003001   0.119948      0.000000   \n",
              "3            0.0        0.0  33.0  0.0     0.000000   0.043035      0.015793   \n",
              "4            1.0        0.0  16.0  0.0     0.021149   0.002348      0.006428   \n",
              "...          ...        ...   ...  ...          ...        ...           ...   \n",
              "8688         0.0        0.0  41.0  1.0     0.000000   0.228726      0.000000   \n",
              "8689         1.0        1.0  18.0  0.0     0.000000   0.000000      0.000000   \n",
              "8690         1.0        0.0  26.0  0.0     0.000000   0.000000      0.079687   \n",
              "8691         0.0        0.0  32.0  0.0     0.000000   0.035186      0.000000   \n",
              "8692         0.0        0.0  44.0  0.0     0.008795   0.157247      0.000000   \n",
              "\n",
              "           Spa    VRDeck  Transported  \n",
              "0     0.000000  0.000000          0.0  \n",
              "1     0.024500  0.002164          1.0  \n",
              "2     0.299670  0.002410          0.0  \n",
              "3     0.148563  0.009491          0.0  \n",
              "4     0.025214  0.000098          1.0  \n",
              "...        ...       ...          ...  \n",
              "8688  0.073322  0.003639          0.0  \n",
              "8689  0.000000  0.000000          0.0  \n",
              "8690  0.000045  0.000000          1.0  \n",
              "8691  0.015753  0.159077          0.0  \n",
              "8692  0.000000  0.000590          1.0  \n",
              "\n",
              "[7074 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca03248d-7ee8-4165-8a08-b69b6a36b2c5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HomePlanet</th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Transported</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007608</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>0.024500</td>\n",
              "      <td>0.002164</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.003001</td>\n",
              "      <td>0.119948</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.299670</td>\n",
              "      <td>0.002410</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043035</td>\n",
              "      <td>0.015793</td>\n",
              "      <td>0.148563</td>\n",
              "      <td>0.009491</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021149</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.006428</td>\n",
              "      <td>0.025214</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.228726</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.073322</td>\n",
              "      <td>0.003639</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079687</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.035186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015753</td>\n",
              "      <td>0.159077</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008795</td>\n",
              "      <td>0.157247</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7074 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca03248d-7ee8-4165-8a08-b69b6a36b2c5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca03248d-7ee8-4165-8a08-b69b6a36b2c5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca03248d-7ee8-4165-8a08-b69b6a36b2c5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df.drop(\"Transported\",axis=1)\n",
        "y_train = train_df[\"Transported\"]\n",
        "\n",
        "X_test = test_df.drop(\"Transported\",axis=1)\n",
        "y_test = test_df[\"Transported\"]"
      ],
      "metadata": {
        "id": "qcxv8pIqhuua"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "E-7AsKJoms1q"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train.values).to(device).to(torch.float32)\n",
        "X_test = torch.tensor(X_test.values).to(device).to(torch.float32)\n",
        "y_train = torch.tensor(y_train.values).to(device).to(torch.float32)\n",
        "y_test = torch.tensor(y_test.values).to(device).to(torch.float32)"
      ],
      "metadata": {
        "id": "fcoSyXD5m9qi"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape,y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-suBs3RlnWff",
        "outputId": "91b0c2df-b65d-4008-f840-d5d65eb7e5b7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([7074, 9]), torch.Size([7074]))"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_size = len(features) - 1\n",
        "hidden_size = 64\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(in_size,hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size,hidden_size),\n",
        "    nn.ELU(),\n",
        "    nn.Linear(hidden_size,hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size,hidden_size),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(hidden_size,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldEVFqHOn06m",
        "outputId": "f548c072-ed9d-4c03-e7e5-3098b625792e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=9, out_features=64, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (3): ELU(alpha=1.0)\n",
              "  (4): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (5): ReLU()\n",
              "  (6): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (7): Tanh()\n",
              "  (8): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (9): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(lr=0.01,params = model.parameters())\n",
        "\n"
      ],
      "metadata": {
        "id": "CXirkN00oW56"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(true_values,predicted_values):\n",
        "  correct = torch.eq(true_values,predicted_values).sum().item()\n",
        "  acc = (correct/len(true_values)) * 100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "krBERgiKqWzy"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "  y_preds = model.forward(X_train).squeeze()\n",
        "  loss = loss_fn(y_preds, y_train)\n",
        "  accuracy = accuracy_fn(y_train,y_preds.round())\n",
        "  \n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    y_preds = model.forward(X_test).squeeze()\n",
        "    test_acc = accuracy_fn(y_test,y_preds.round())\n",
        "  if test_acc  >= 90:\n",
        "    break\n",
        "  print(f\"Epoch:[{epoch+1}/{epochs}]| Loss: {loss}| Train Accuracy: {accuracy}% | Test Accuracy: {test_acc}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNznw0rnoYCK",
        "outputId": "f0134963-d765-442f-bc36-a6c6c5aef1f5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:[1/1000]| Loss: 0.7036399245262146| Train Accuracy: 50.26858919988692% | Test Accuracy: 47.86081003993154%\n",
            "Epoch:[2/1000]| Loss: 0.8361844420433044| Train Accuracy: 49.73141080011309% | Test Accuracy: 52.13918996006846%\n",
            "Epoch:[3/1000]| Loss: 0.7800620198249817| Train Accuracy: 50.26858919988692% | Test Accuracy: 51.3405590416429%\n",
            "Epoch:[4/1000]| Loss: 0.6920421123504639| Train Accuracy: 51.08849307322589% | Test Accuracy: 51.59726183685112%\n",
            "Epoch:[5/1000]| Loss: 0.709585964679718| Train Accuracy: 52.756573367260394% | Test Accuracy: 51.968054763262984%\n",
            "Epoch:[6/1000]| Loss: 0.6929411292076111| Train Accuracy: 53.18066157760815% | Test Accuracy: 52.13918996006846%\n",
            "Epoch:[7/1000]| Loss: 0.6869352459907532| Train Accuracy: 50.26858919988692% | Test Accuracy: 52.13918996006846%\n",
            "Epoch:[8/1000]| Loss: 0.6906390190124512| Train Accuracy: 50.26858919988692% | Test Accuracy: 52.13918996006846%\n",
            "Epoch:[9/1000]| Loss: 0.684993326663971| Train Accuracy: 50.26858919988692% | Test Accuracy: 52.538505419281236%\n",
            "Epoch:[10/1000]| Loss: 0.6819913387298584| Train Accuracy: 53.64715860899067% | Test Accuracy: 51.9965772960639%\n",
            "Epoch:[11/1000]| Loss: 0.6841486096382141| Train Accuracy: 53.23720667232118% | Test Accuracy: 51.9965772960639%\n",
            "Epoch:[12/1000]| Loss: 0.6856934428215027| Train Accuracy: 53.23720667232118% | Test Accuracy: 52.22475755847119%\n",
            "Epoch:[13/1000]| Loss: 0.6841809749603271| Train Accuracy: 53.42097823013854% | Test Accuracy: 52.31032515687393%\n",
            "Epoch:[14/1000]| Loss: 0.6827235817909241| Train Accuracy: 53.44925077749505% | Test Accuracy: 52.65259555048488%\n",
            "Epoch:[15/1000]| Loss: 0.6833164691925049| Train Accuracy: 54.099519366694935% | Test Accuracy: 52.13918996006846%\n",
            "Epoch:[16/1000]| Loss: 0.6838774085044861| Train Accuracy: 50.26858919988692% | Test Accuracy: 52.31032515687393%\n",
            "Epoch:[17/1000]| Loss: 0.6828377842903137| Train Accuracy: 50.81990387333899% | Test Accuracy: 54.0501996577296%\n",
            "Epoch:[18/1000]| Loss: 0.6816579103469849| Train Accuracy: 54.80633305060786% | Test Accuracy: 53.02338847689675%\n",
            "Epoch:[19/1000]| Loss: 0.6816101670265198| Train Accuracy: 54.000565450947136% | Test Accuracy: 52.88077581289219%\n",
            "Epoch:[20/1000]| Loss: 0.6819900274276733| Train Accuracy: 53.8733389878428% | Test Accuracy: 53.30861380490588%\n",
            "Epoch:[21/1000]| Loss: 0.681512176990509| Train Accuracy: 54.28329092451229% | Test Accuracy: 53.90758699372504%\n",
            "Epoch:[22/1000]| Loss: 0.6807826161384583| Train Accuracy: 54.749787955894824% | Test Accuracy: 52.56702795208214%\n",
            "Epoch:[23/1000]| Loss: 0.6817250847816467| Train Accuracy: 53.25134294599943% | Test Accuracy: 53.50827153451226%\n",
            "Epoch:[24/1000]| Loss: 0.6803712248802185| Train Accuracy: 54.36810856658185% | Test Accuracy: 55.3907586993725%\n",
            "Epoch:[25/1000]| Loss: 0.6800785660743713| Train Accuracy: 55.58382810291207% | Test Accuracy: 56.0467769537935%\n",
            "Epoch:[26/1000]| Loss: 0.6796967387199402| Train Accuracy: 55.923098671190274% | Test Accuracy: 55.81859669138619%\n",
            "Epoch:[27/1000]| Loss: 0.6782826781272888| Train Accuracy: 55.8806898501555% | Test Accuracy: 54.620650313747866%\n",
            "Epoch:[28/1000]| Loss: 0.6785131692886353| Train Accuracy: 54.820469324286115% | Test Accuracy: 56.41756988020536%\n",
            "Epoch:[29/1000]| Loss: 0.6761848330497742| Train Accuracy: 56.34718688153802% | Test Accuracy: 57.81517398745009%\n",
            "Epoch:[30/1000]| Loss: 0.6752830147743225| Train Accuracy: 57.1388182075205% | Test Accuracy: 57.529948659440954%\n",
            "Epoch:[31/1000]| Loss: 0.6719951033592224| Train Accuracy: 57.70426915465083% | Test Accuracy: 58.55675984027382%\n",
            "Epoch:[32/1000]| Loss: 0.6683748960494995| Train Accuracy: 58.60899067005937% | Test Accuracy: 60.38220193953223%\n",
            "Epoch:[33/1000]| Loss: 0.6657779812812805| Train Accuracy: 58.919988690981064% | Test Accuracy: 59.18425556189389%\n",
            "Epoch:[34/1000]| Loss: 0.6659258604049683| Train Accuracy: 56.09273395532938% | Test Accuracy: 56.18938961779806%\n",
            "Epoch:[35/1000]| Loss: 0.6801743507385254| Train Accuracy: 56.40373197625106% | Test Accuracy: 55.67598402738163%\n",
            "Epoch:[36/1000]| Loss: 0.6787884831428528| Train Accuracy: 56.14927905004241% | Test Accuracy: 67.74101540216772%\n",
            "Epoch:[37/1000]| Loss: 0.6503890752792358| Train Accuracy: 64.277636415041% | Test Accuracy: 52.13918996006846%\n",
            "Epoch:[38/1000]| Loss: 0.7613508701324463| Train Accuracy: 50.26858919988692% | Test Accuracy: 69.88020536223617%\n",
            "Epoch:[39/1000]| Loss: 0.6456534266471863| Train Accuracy: 66.22844218264066% | Test Accuracy: 56.07529948659441%\n",
            "Epoch:[40/1000]| Loss: 0.6745589971542358| Train Accuracy: 56.318914334181514% | Test Accuracy: 55.077010838562465%\n",
            "Epoch:[41/1000]| Loss: 0.6768784523010254| Train Accuracy: 55.51314673452078% | Test Accuracy: 63.40559041642898%\n",
            "Epoch:[42/1000]| Loss: 0.6549869775772095| Train Accuracy: 61.111111111111114% | Test Accuracy: 57.01654306902453%\n",
            "Epoch:[43/1000]| Loss: 0.6534913778305054| Train Accuracy: 54.12779191405146% | Test Accuracy: 52.22475755847119%\n",
            "Epoch:[44/1000]| Loss: 0.6621222496032715| Train Accuracy: 50.28272547356517% | Test Accuracy: 77.8094694808899%\n",
            "Epoch:[45/1000]| Loss: 0.6415674686431885| Train Accuracy: 71.09132032796155% | Test Accuracy: 68.65373645179692%\n",
            "Epoch:[46/1000]| Loss: 0.645000159740448| Train Accuracy: 65.22476675148431% | Test Accuracy: 70.73588134626355%\n",
            "Epoch:[47/1000]| Loss: 0.6354179978370667| Train Accuracy: 66.75148430873622% | Test Accuracy: 80.63320022818026%\n",
            "Epoch:[48/1000]| Loss: 0.614310085773468| Train Accuracy: 73.4520780322307% | Test Accuracy: 67.62692527096407%\n",
            "Epoch:[49/1000]| Loss: 0.6151046752929688| Train Accuracy: 63.2880972575629% | Test Accuracy: 75.86993725042784%\n",
            "Epoch:[50/1000]| Loss: 0.5975838899612427| Train Accuracy: 70.58241447554424% | Test Accuracy: 79.12150598973189%\n",
            "Epoch:[51/1000]| Loss: 0.5750697255134583| Train Accuracy: 72.39185750636132% | Test Accuracy: 59.612093553907584%\n",
            "Epoch:[52/1000]| Loss: 0.6291700601577759| Train Accuracy: 55.41419281877297% | Test Accuracy: 71.22076440387907%\n",
            "Epoch:[53/1000]| Loss: 0.6239920258522034| Train Accuracy: 67.232117613797% | Test Accuracy: 69.11009697661153%\n",
            "Epoch:[54/1000]| Loss: 0.6409063339233398| Train Accuracy: 65.63471868815381% | Test Accuracy: 81.68853394181403%\n",
            "Epoch:[55/1000]| Loss: 0.5485077500343323| Train Accuracy: 74.52643483177835% | Test Accuracy: 56.13234455219624%\n",
            "Epoch:[56/1000]| Loss: 0.7105279564857483| Train Accuracy: 52.79898218829516% | Test Accuracy: 81.68853394181403%\n",
            "Epoch:[57/1000]| Loss: 0.5499060153961182| Train Accuracy: 74.34266327396098% | Test Accuracy: 70.25099828864803%\n",
            "Epoch:[58/1000]| Loss: 0.614108681678772| Train Accuracy: 66.63839411931015% | Test Accuracy: 67.42726754135766%\n",
            "Epoch:[59/1000]| Loss: 0.6274092197418213| Train Accuracy: 64.46140797285835% | Test Accuracy: 73.18881916714204%\n",
            "Epoch:[60/1000]| Loss: 0.5883470177650452| Train Accuracy: 68.99915182357931% | Test Accuracy: 81.1751283513976%\n",
            "Epoch:[61/1000]| Loss: 0.5650579929351807| Train Accuracy: 74.1306191687871% | Test Accuracy: 77.780946948089%\n",
            "Epoch:[62/1000]| Loss: 0.5895757079124451| Train Accuracy: 70.4975968334747% | Test Accuracy: 77.1534512264689%\n",
            "Epoch:[63/1000]| Loss: 0.5915657877922058| Train Accuracy: 69.77664687588351% | Test Accuracy: 83.4284084426697%\n",
            "Epoch:[64/1000]| Loss: 0.5600193738937378| Train Accuracy: 76.80237489397794% | Test Accuracy: 80.83285795778666%\n",
            "Epoch:[65/1000]| Loss: 0.5518146753311157| Train Accuracy: 73.91857506361323% | Test Accuracy: 79.20707358813462%\n",
            "Epoch:[66/1000]| Loss: 0.554425060749054| Train Accuracy: 72.85835453774385% | Test Accuracy: 80.11979463776383%\n",
            "Epoch:[67/1000]| Loss: 0.5534930229187012| Train Accuracy: 73.15521628498728% | Test Accuracy: 81.28921848260126%\n",
            "Epoch:[68/1000]| Loss: 0.5471521615982056| Train Accuracy: 74.11648289510885% | Test Accuracy: 84.02738163148888%\n",
            "Epoch:[69/1000]| Loss: 0.5367441773414612| Train Accuracy: 76.39242295730845% | Test Accuracy: 83.8277239018825%\n",
            "Epoch:[70/1000]| Loss: 0.522763192653656| Train Accuracy: 76.40655923098672% | Test Accuracy: 82.82943525385055%\n",
            "Epoch:[71/1000]| Loss: 0.5304840207099915| Train Accuracy: 75.82697201017811% | Test Accuracy: 79.1500285225328%\n",
            "Epoch:[72/1000]| Loss: 0.5568056702613831| Train Accuracy: 72.03845066440486% | Test Accuracy: 81.08956075299487%\n",
            "Epoch:[73/1000]| Loss: 0.5607103705406189| Train Accuracy: 74.04580152671755% | Test Accuracy: 81.00399315459212%\n",
            "Epoch:[74/1000]| Loss: 0.5643965005874634| Train Accuracy: 73.960983884648% | Test Accuracy: 84.08442669709069%\n",
            "Epoch:[75/1000]| Loss: 0.5180380344390869| Train Accuracy: 76.84478371501272% | Test Accuracy: 66.31488876212208%\n",
            "Epoch:[76/1000]| Loss: 0.6464787125587463| Train Accuracy: 60.729431721798136% | Test Accuracy: 81.43183114660582%\n",
            "Epoch:[77/1000]| Loss: 0.5557358264923096| Train Accuracy: 74.11648289510885% | Test Accuracy: 74.15858528237308%\n",
            "Epoch:[78/1000]| Loss: 0.6264326572418213| Train Accuracy: 70.00282725473565% | Test Accuracy: 72.61836851112379%\n",
            "Epoch:[79/1000]| Loss: 0.6268311142921448| Train Accuracy: 68.61747243426632% | Test Accuracy: 76.86822589845978%\n",
            "Epoch:[80/1000]| Loss: 0.5711408257484436| Train Accuracy: 71.45886344359627% | Test Accuracy: 81.83114660581859%\n",
            "Epoch:[81/1000]| Loss: 0.5573557615280151| Train Accuracy: 74.3709358213175% | Test Accuracy: 84.25556189389619%\n",
            "Epoch:[82/1000]| Loss: 0.5815222859382629| Train Accuracy: 74.85156912637828% | Test Accuracy: 82.9435253850542%\n",
            "Epoch:[83/1000]| Loss: 0.5846801996231079| Train Accuracy: 73.9327113372915% | Test Accuracy: 83.25727324586423%\n",
            "Epoch:[84/1000]| Loss: 0.5634605288505554| Train Accuracy: 75.60079163132598% | Test Accuracy: 83.68511123787792%\n",
            "Epoch:[85/1000]| Loss: 0.5426622629165649| Train Accuracy: 77.14164546225615% | Test Accuracy: 83.05761551625784%\n",
            "Epoch:[86/1000]| Loss: 0.5351676940917969| Train Accuracy: 76.2793327678824% | Test Accuracy: 83.00057045065601%\n",
            "Epoch:[87/1000]| Loss: 0.5327423810958862| Train Accuracy: 76.16624257845632% | Test Accuracy: 83.45693097547063%\n",
            "Epoch:[88/1000]| Loss: 0.5276608467102051| Train Accuracy: 76.9861464517953% | Test Accuracy: 83.1717056474615%\n",
            "Epoch:[89/1000]| Loss: 0.528069257736206| Train Accuracy: 76.33587786259542% | Test Accuracy: 82.7438676554478%\n",
            "Epoch:[90/1000]| Loss: 0.5293096899986267| Train Accuracy: 75.8693808312129% | Test Accuracy: 83.45693097547063%\n",
            "Epoch:[91/1000]| Loss: 0.5260083675384521| Train Accuracy: 76.64687588351711% | Test Accuracy: 83.37136337706788%\n",
            "Epoch:[92/1000]| Loss: 0.5254441499710083| Train Accuracy: 77.11337291489964% | Test Accuracy: 83.54249857387336%\n",
            "Epoch:[93/1000]| Loss: 0.5231091976165771| Train Accuracy: 77.11337291489964% | Test Accuracy: 83.88476896748432%\n",
            "Epoch:[94/1000]| Loss: 0.5178258419036865| Train Accuracy: 77.04269154650834% | Test Accuracy: 83.51397604107245%\n",
            "Epoch:[95/1000]| Loss: 0.5180665254592896| Train Accuracy: 76.29346904156064% | Test Accuracy: 83.97033656588705%\n",
            "Epoch:[96/1000]| Loss: 0.5146024227142334| Train Accuracy: 76.7175572519084% | Test Accuracy: 84.08442669709069%\n",
            "Epoch:[97/1000]| Loss: 0.5129538178443909| Train Accuracy: 77.2264631043257% | Test Accuracy: 84.16999429549344%\n",
            "Epoch:[98/1000]| Loss: 0.5119429230690002| Train Accuracy: 77.07096409386486% | Test Accuracy: 84.76896748431261%\n",
            "Epoch:[99/1000]| Loss: 0.5086073279380798| Train Accuracy: 77.0568278201866% | Test Accuracy: 84.76896748431261%\n",
            "Epoch:[100/1000]| Loss: 0.5066966414451599| Train Accuracy: 76.67514843087362% | Test Accuracy: 83.91329150028523%\n",
            "Epoch:[101/1000]| Loss: 0.5092799067497253| Train Accuracy: 76.81651116765622% | Test Accuracy: 84.79749001711352%\n",
            "Epoch:[102/1000]| Loss: 0.505751371383667| Train Accuracy: 76.42069550466498% | Test Accuracy: 85.22532800912721%\n",
            "Epoch:[103/1000]| Loss: 0.5037464499473572| Train Accuracy: 76.94373763076054% | Test Accuracy: 84.9971477467199%\n",
            "Epoch:[104/1000]| Loss: 0.4978143870830536| Train Accuracy: 77.11337291489964% | Test Accuracy: 85.53907586993725%\n",
            "Epoch:[105/1000]| Loss: 0.4921613335609436| Train Accuracy: 76.77410234662143% | Test Accuracy: 87.16486023958927%\n",
            "Epoch:[106/1000]| Loss: 0.49575334787368774| Train Accuracy: 76.74582979926491% | Test Accuracy: 84.94010268111808%\n",
            "Epoch:[107/1000]| Loss: 0.5040031671524048| Train Accuracy: 76.90132880972575% | Test Accuracy: 85.79577866514546%\n",
            "Epoch:[108/1000]| Loss: 0.4891744554042816| Train Accuracy: 77.2264631043257% | Test Accuracy: 85.79577866514546%\n",
            "Epoch:[109/1000]| Loss: 0.5130274295806885| Train Accuracy: 76.64687588351711% | Test Accuracy: 80.83285795778666%\n",
            "Epoch:[110/1000]| Loss: 0.5764371156692505| Train Accuracy: 74.1023466214306% | Test Accuracy: 83.25727324586423%\n",
            "Epoch:[111/1000]| Loss: 0.5509988069534302| Train Accuracy: 75.51597398925644% | Test Accuracy: 87.47860810039931%\n",
            "Epoch:[112/1000]| Loss: 0.4961458444595337| Train Accuracy: 78.2442748091603% | Test Accuracy: 87.02224757558471%\n",
            "Epoch:[113/1000]| Loss: 0.5115092396736145| Train Accuracy: 78.30081990387335% | Test Accuracy: 84.05590416428979%\n",
            "Epoch:[114/1000]| Loss: 0.5265184044837952| Train Accuracy: 75.91178965224766% | Test Accuracy: 83.08613804905876%\n",
            "Epoch:[115/1000]| Loss: 0.5343770384788513| Train Accuracy: 75.38874752615212% | Test Accuracy: 86.1095265259555%\n",
            "Epoch:[116/1000]| Loss: 0.4850485920906067| Train Accuracy: 77.3254170200735% | Test Accuracy: 80.74729035938391%\n",
            "Epoch:[117/1000]| Loss: 0.5426996350288391| Train Accuracy: 72.56149279050042% | Test Accuracy: 86.73702224757558%\n",
            "Epoch:[118/1000]| Loss: 0.484575092792511| Train Accuracy: 77.45264348317782% | Test Accuracy: 83.28579577866515%\n",
            "Epoch:[119/1000]| Loss: 0.5043932795524597| Train Accuracy: 75.96833474696069% | Test Accuracy: 83.59954363947519%\n",
            "Epoch:[120/1000]| Loss: 0.5033524036407471| Train Accuracy: 75.96833474696069% | Test Accuracy: 85.59612093553908%\n",
            "Epoch:[121/1000]| Loss: 0.4914218485355377| Train Accuracy: 77.33955329375176% | Test Accuracy: 87.67826583000571%\n",
            "Epoch:[122/1000]| Loss: 0.490248441696167| Train Accuracy: 77.35368956743002% | Test Accuracy: 87.39304050199658%\n",
            "Epoch:[123/1000]| Loss: 0.4900761544704437| Train Accuracy: 77.38196211478655% | Test Accuracy: 85.82430119794637%\n",
            "Epoch:[124/1000]| Loss: 0.4908684194087982| Train Accuracy: 77.36782584110829% | Test Accuracy: 85.51055333713634%\n",
            "Epoch:[125/1000]| Loss: 0.48341482877731323| Train Accuracy: 77.35368956743002% | Test Accuracy: 86.62293211637193%\n",
            "Epoch:[126/1000]| Loss: 0.4728795289993286| Train Accuracy: 77.36782584110829% | Test Accuracy: 87.47860810039931%\n",
            "Epoch:[127/1000]| Loss: 0.48624974489212036| Train Accuracy: 77.2264631043257% | Test Accuracy: 86.50884198516829%\n",
            "Epoch:[128/1000]| Loss: 0.4683407247066498| Train Accuracy: 77.4950523042126% | Test Accuracy: 86.59440958357102%\n",
            "Epoch:[129/1000]| Loss: 0.4686928391456604| Train Accuracy: 77.52332485156913% | Test Accuracy: 87.4500855675984%\n",
            "Epoch:[130/1000]| Loss: 0.47380900382995605| Train Accuracy: 78.00395815662992% | Test Accuracy: 87.25042783799202%\n",
            "Epoch:[131/1000]| Loss: 0.45863136649131775| Train Accuracy: 78.07463952502121% | Test Accuracy: 87.36451796919566%\n",
            "Epoch:[132/1000]| Loss: 0.4575565755367279| Train Accuracy: 77.91914051456035% | Test Accuracy: 86.82258984597833%\n",
            "Epoch:[133/1000]| Loss: 0.46198299527168274| Train Accuracy: 78.55527283008199% | Test Accuracy: 87.30747290359385%\n",
            "Epoch:[134/1000]| Loss: 0.4610350728034973| Train Accuracy: 77.735368956743% | Test Accuracy: 87.73531089560753%\n",
            "Epoch:[135/1000]| Loss: 0.4526606500148773| Train Accuracy: 79.38931297709924% | Test Accuracy: 87.73531089560753%\n",
            "Epoch:[136/1000]| Loss: 0.4524616599082947| Train Accuracy: 78.01809443030817% | Test Accuracy: 87.4500855675984%\n",
            "Epoch:[137/1000]| Loss: 0.45268872380256653| Train Accuracy: 79.43172179813402% | Test Accuracy: 86.70849971477466%\n",
            "Epoch:[138/1000]| Loss: 0.48981404304504395| Train Accuracy: 77.11337291489964% | Test Accuracy: 86.22361665715916%\n",
            "Epoch:[139/1000]| Loss: 0.4659460186958313| Train Accuracy: 78.35736499858638% | Test Accuracy: 86.5658870507701%\n",
            "Epoch:[140/1000]| Loss: 0.4904564917087555| Train Accuracy: 76.94373763076054% | Test Accuracy: 87.99201369081574%\n",
            "Epoch:[141/1000]| Loss: 0.44625550508499146| Train Accuracy: 80.16680802940344% | Test Accuracy: 88.3628066172276%\n",
            "Epoch:[142/1000]| Loss: 0.4426829516887665| Train Accuracy: 80.11026293469041% | Test Accuracy: 87.05077010838562%\n",
            "Epoch:[143/1000]| Loss: 0.4611731469631195| Train Accuracy: 77.55159739892564% | Test Accuracy: 87.99201369081574%\n",
            "Epoch:[144/1000]| Loss: 0.4466487169265747| Train Accuracy: 79.9123551031948% | Test Accuracy: 87.99201369081574%\n",
            "Epoch:[145/1000]| Loss: 0.4378482401371002| Train Accuracy: 78.56940910376025% | Test Accuracy: 88.13462635482031%\n",
            "Epoch:[146/1000]| Loss: 0.4399856925010681| Train Accuracy: 78.20186598812553% | Test Accuracy: 87.93496862521391%\n",
            "Epoch:[147/1000]| Loss: 0.4440859854221344| Train Accuracy: 79.88408255583828% | Test Accuracy: 87.99201369081574%\n",
            "Epoch:[148/1000]| Loss: 0.4427327513694763| Train Accuracy: 78.20186598812553% | Test Accuracy: 88.22019395322305%\n",
            "Epoch:[149/1000]| Loss: 0.4330771863460541| Train Accuracy: 79.71444727169919% | Test Accuracy: 88.50541928123218%\n",
            "Epoch:[150/1000]| Loss: 0.43416422605514526| Train Accuracy: 80.42126095561211% | Test Accuracy: 88.02053622361666%\n",
            "Epoch:[151/1000]| Loss: 0.4452807903289795| Train Accuracy: 78.2442748091603% | Test Accuracy: 86.42327438676554%\n",
            "Epoch:[152/1000]| Loss: 0.44836780428886414| Train Accuracy: 78.90867967203845% | Test Accuracy: 87.70678836280662%\n",
            "Epoch:[153/1000]| Loss: 0.46624666452407837| Train Accuracy: 77.7070964093865% | Test Accuracy: 88.13462635482031%\n",
            "Epoch:[154/1000]| Loss: 0.4331294894218445| Train Accuracy: 80.26576194515125% | Test Accuracy: 88.53394181403308%\n",
            "Epoch:[155/1000]| Loss: 0.4279711842536926| Train Accuracy: 80.25162567147301% | Test Accuracy: 88.27723901882487%\n",
            "Epoch:[156/1000]| Loss: 0.43976345658302307| Train Accuracy: 78.5835453774385% | Test Accuracy: 86.3091842555619%\n",
            "Epoch:[157/1000]| Loss: 0.44873085618019104| Train Accuracy: 78.5835453774385% | Test Accuracy: 87.2219053051911%\n",
            "Epoch:[158/1000]| Loss: 0.4695492088794708| Train Accuracy: 77.69296013570822% | Test Accuracy: 88.562464346834%\n",
            "Epoch:[159/1000]| Loss: 0.4270760715007782| Train Accuracy: 79.9406276505513% | Test Accuracy: 84.91158014831717%\n",
            "Epoch:[160/1000]| Loss: 0.46017828583717346| Train Accuracy: 77.43850720949958% | Test Accuracy: 86.0810039931546%\n",
            "Epoch:[161/1000]| Loss: 0.5044978857040405| Train Accuracy: 76.74582979926491% | Test Accuracy: 87.13633770678835%\n",
            "Epoch:[162/1000]| Loss: 0.45630738139152527| Train Accuracy: 77.79191405145603% | Test Accuracy: 77.95208214489446%\n",
            "Epoch:[163/1000]| Loss: 0.52571040391922| Train Accuracy: 69.8190556969183% | Test Accuracy: 88.3342840844267%\n",
            "Epoch:[164/1000]| Loss: 0.4343109726905823| Train Accuracy: 79.2338139666384% | Test Accuracy: 85.90986879634912%\n",
            "Epoch:[165/1000]| Loss: 0.4780418574810028| Train Accuracy: 76.56205824144755% | Test Accuracy: 86.62293211637193%\n",
            "Epoch:[166/1000]| Loss: 0.4561569094657898| Train Accuracy: 77.08510036754312% | Test Accuracy: 88.47689674843126%\n",
            "Epoch:[167/1000]| Loss: 0.451802134513855| Train Accuracy: 79.4034492507775% | Test Accuracy: 87.39304050199658%\n",
            "Epoch:[168/1000]| Loss: 0.47130465507507324| Train Accuracy: 79.78512864009048% | Test Accuracy: 88.64803194523674%\n",
            "Epoch:[169/1000]| Loss: 0.449203759431839| Train Accuracy: 78.73904438789936% | Test Accuracy: 86.7655447803765%\n",
            "Epoch:[170/1000]| Loss: 0.4550755023956299| Train Accuracy: 77.18405428329092% | Test Accuracy: 86.02395892755277%\n",
            "Epoch:[171/1000]| Loss: 0.4602610766887665| Train Accuracy: 76.91546508340402% | Test Accuracy: 87.79235596120935%\n",
            "Epoch:[172/1000]| Loss: 0.44319948554039| Train Accuracy: 77.82018659881255% | Test Accuracy: 88.07758128921849%\n",
            "Epoch:[173/1000]| Loss: 0.4536365866661072| Train Accuracy: 79.4034492507775% | Test Accuracy: 87.99201369081574%\n",
            "Epoch:[174/1000]| Loss: 0.4408349096775055| Train Accuracy: 79.41758552445576% | Test Accuracy: 88.87621220764403%\n",
            "Epoch:[175/1000]| Loss: 0.4423986077308655| Train Accuracy: 78.45631891433418% | Test Accuracy: 88.64803194523674%\n",
            "Epoch:[176/1000]| Loss: 0.4372652769088745| Train Accuracy: 78.42804636697767% | Test Accuracy: 88.16314888762122%\n",
            "Epoch:[177/1000]| Loss: 0.4319361448287964| Train Accuracy: 79.78512864009048% | Test Accuracy: 88.47689674843126%\n",
            "Epoch:[178/1000]| Loss: 0.4333682954311371| Train Accuracy: 80.06785411365564% | Test Accuracy: 88.87621220764403%\n",
            "Epoch:[179/1000]| Loss: 0.43601173162460327| Train Accuracy: 78.78145320893412% | Test Accuracy: 88.44837421563034%\n",
            "Epoch:[180/1000]| Loss: 0.42685067653656006| Train Accuracy: 79.99717274526435% | Test Accuracy: 87.02224757558471%\n",
            "Epoch:[181/1000]| Loss: 0.4400292932987213| Train Accuracy: 79.10658750353406% | Test Accuracy: 88.1061038220194%\n",
            "Epoch:[182/1000]| Loss: 0.46049296855926514| Train Accuracy: 78.2442748091603% | Test Accuracy: 88.59098687963491%\n",
            "Epoch:[183/1000]| Loss: 0.42386767268180847| Train Accuracy: 79.78512864009048% | Test Accuracy: 83.74215630347976%\n",
            "Epoch:[184/1000]| Loss: 0.47728538513183594| Train Accuracy: 75.8976533785694% | Test Accuracy: 86.05248146035368%\n",
            "Epoch:[185/1000]| Loss: 0.5299077033996582| Train Accuracy: 76.7175572519084% | Test Accuracy: 86.25213918996006%\n",
            "Epoch:[186/1000]| Loss: 0.507623016834259| Train Accuracy: 77.00028272547357% | Test Accuracy: 85.79577866514546%\n",
            "Epoch:[187/1000]| Loss: 0.45401597023010254| Train Accuracy: 78.44218264065591% | Test Accuracy: 84.08442669709069%\n",
            "Epoch:[188/1000]| Loss: 0.4803789258003235| Train Accuracy: 75.84110828385637% | Test Accuracy: 87.76383342840845%\n",
            "Epoch:[189/1000]| Loss: 0.450132817029953| Train Accuracy: 78.23013853548206% | Test Accuracy: 86.19509412435823%\n",
            "Epoch:[190/1000]| Loss: 0.48027414083480835| Train Accuracy: 76.80237489397794% | Test Accuracy: 87.36451796919566%\n",
            "Epoch:[191/1000]| Loss: 0.45219674706459045| Train Accuracy: 77.65055131467345% | Test Accuracy: 88.02053622361666%\n",
            "Epoch:[192/1000]| Loss: 0.4511703550815582| Train Accuracy: 79.65790217698616% | Test Accuracy: 86.39475185396464%\n",
            "Epoch:[193/1000]| Loss: 0.4722340703010559| Train Accuracy: 78.9228159457167% | Test Accuracy: 88.47689674843126%\n",
            "Epoch:[194/1000]| Loss: 0.4513922333717346| Train Accuracy: 79.37517670342098% | Test Accuracy: 87.39304050199658%\n",
            "Epoch:[195/1000]| Loss: 0.4489372968673706| Train Accuracy: 77.67882386202997% | Test Accuracy: 86.3091842555619%\n",
            "Epoch:[196/1000]| Loss: 0.46064746379852295| Train Accuracy: 76.95787390443878% | Test Accuracy: 87.10781517398745%\n",
            "Epoch:[197/1000]| Loss: 0.45168623328208923| Train Accuracy: 77.57986994628216% | Test Accuracy: 88.64803194523674%\n",
            "Epoch:[198/1000]| Loss: 0.4452100992202759| Train Accuracy: 78.49872773536896% | Test Accuracy: 88.7906446092413%\n",
            "Epoch:[199/1000]| Loss: 0.4463164508342743| Train Accuracy: 79.60135708227311% | Test Accuracy: 88.44837421563034%\n",
            "Epoch:[200/1000]| Loss: 0.44300320744514465| Train Accuracy: 79.65790217698616% | Test Accuracy: 88.562464346834%\n",
            "Epoch:[201/1000]| Loss: 0.44167059659957886| Train Accuracy: 78.73904438789936% | Test Accuracy: 87.87792355961209%\n",
            "Epoch:[202/1000]| Loss: 0.44185173511505127| Train Accuracy: 78.35736499858638% | Test Accuracy: 88.59098687963491%\n",
            "Epoch:[203/1000]| Loss: 0.4313955008983612| Train Accuracy: 78.86627085100368% | Test Accuracy: 88.8191671420422%\n",
            "Epoch:[204/1000]| Loss: 0.43698933720588684| Train Accuracy: 79.99717274526435% | Test Accuracy: 88.8191671420422%\n",
            "Epoch:[205/1000]| Loss: 0.4318796396255493| Train Accuracy: 79.78512864009048% | Test Accuracy: 88.61950941243583%\n",
            "Epoch:[206/1000]| Loss: 0.4315345883369446| Train Accuracy: 78.88040712468194% | Test Accuracy: 88.27723901882487%\n",
            "Epoch:[207/1000]| Loss: 0.426628440618515| Train Accuracy: 79.53067571388182% | Test Accuracy: 87.79235596120935%\n",
            "Epoch:[208/1000]| Loss: 0.4332454800605774| Train Accuracy: 79.81340118744698% | Test Accuracy: 88.84768967484312%\n",
            "Epoch:[209/1000]| Loss: 0.4249005615711212| Train Accuracy: 80.01130901894261% | Test Accuracy: 88.59098687963491%\n",
            "Epoch:[210/1000]| Loss: 0.42610806226730347| Train Accuracy: 79.62962962962963% | Test Accuracy: 88.30576155162578%\n",
            "Epoch:[211/1000]| Loss: 0.4264233708381653| Train Accuracy: 80.05371783997738% | Test Accuracy: 89.07586993725043%\n",
            "Epoch:[212/1000]| Loss: 0.42294546961784363| Train Accuracy: 80.25162567147301% | Test Accuracy: 89.13291500285226%\n",
            "Epoch:[213/1000]| Loss: 0.42337721586227417| Train Accuracy: 79.71444727169919% | Test Accuracy: 87.99201369081574%\n",
            "Epoch:[214/1000]| Loss: 0.4237532317638397| Train Accuracy: 80.06785411365564% | Test Accuracy: 88.7906446092413%\n",
            "Epoch:[215/1000]| Loss: 0.4220500886440277| Train Accuracy: 80.39298840825559% | Test Accuracy: 88.73359954363947%\n",
            "Epoch:[216/1000]| Loss: 0.42032861709594727| Train Accuracy: 80.49194232400339% | Test Accuracy: 88.13462635482031%\n",
            "Epoch:[217/1000]| Loss: 0.4222121834754944| Train Accuracy: 80.16680802940344% | Test Accuracy: 89.16143753565316%\n",
            "Epoch:[218/1000]| Loss: 0.4239232838153839| Train Accuracy: 79.70031099802092% | Test Accuracy: 88.53394181403308%\n",
            "Epoch:[219/1000]| Loss: 0.4202936291694641| Train Accuracy: 80.33644331354255% | Test Accuracy: 88.96177980604678%\n",
            "Epoch:[220/1000]| Loss: 0.41903916001319885| Train Accuracy: 80.57675996607294% | Test Accuracy: 89.53223046206503%\n",
            "Epoch:[221/1000]| Loss: 0.41977658867836| Train Accuracy: 80.11026293469041% | Test Accuracy: 88.59098687963491%\n",
            "Epoch:[222/1000]| Loss: 0.4213816523551941| Train Accuracy: 80.1526717557252% | Test Accuracy: 89.4466628636623%\n",
            "Epoch:[223/1000]| Loss: 0.4224832355976105| Train Accuracy: 80.0819903873339% | Test Accuracy: 88.7906446092413%\n",
            "Epoch:[224/1000]| Loss: 0.41929084062576294| Train Accuracy: 80.46366977664687% | Test Accuracy: 88.96177980604678%\n",
            "Epoch:[225/1000]| Loss: 0.41897517442703247| Train Accuracy: 80.53435114503816% | Test Accuracy: 89.27552766685682%\n",
            "Epoch:[226/1000]| Loss: 0.4202738106250763| Train Accuracy: 79.64376590330788% | Test Accuracy: 88.59098687963491%\n",
            "Epoch:[227/1000]| Loss: 0.4194987118244171| Train Accuracy: 80.1526717557252% | Test Accuracy: 89.10439247005134%\n",
            "Epoch:[228/1000]| Loss: 0.417272686958313| Train Accuracy: 80.5908962397512% | Test Accuracy: 89.41814033086139%\n",
            "Epoch:[229/1000]| Loss: 0.41778382658958435| Train Accuracy: 79.9406276505513% | Test Accuracy: 88.64803194523674%\n",
            "Epoch:[230/1000]| Loss: 0.419247567653656| Train Accuracy: 80.37885213457733% | Test Accuracy: 89.4751853964632%\n",
            "Epoch:[231/1000]| Loss: 0.4180125892162323| Train Accuracy: 79.99717274526435% | Test Accuracy: 89.13291500285226%\n",
            "Epoch:[232/1000]| Loss: 0.4168463349342346| Train Accuracy: 80.43539722929036% | Test Accuracy: 88.93325727324587%\n",
            "Epoch:[233/1000]| Loss: 0.4167528748512268| Train Accuracy: 80.60503251342946% | Test Accuracy: 89.4466628636623%\n",
            "Epoch:[234/1000]| Loss: 0.41844502091407776| Train Accuracy: 79.71444727169919% | Test Accuracy: 88.96177980604678%\n",
            "Epoch:[235/1000]| Loss: 0.4159530997276306| Train Accuracy: 80.6615776081425% | Test Accuracy: 88.9903023388477%\n",
            "Epoch:[236/1000]| Loss: 0.4160782992839813| Train Accuracy: 80.61916878710772% | Test Accuracy: 89.33257273245864%\n",
            "Epoch:[237/1000]| Loss: 0.41624850034713745| Train Accuracy: 80.02544529262087% | Test Accuracy: 89.0188248716486%\n",
            "Epoch:[238/1000]| Loss: 0.417066365480423| Train Accuracy: 80.6615776081425% | Test Accuracy: 89.56075299486595%\n",
            "Epoch:[239/1000]| Loss: 0.4159604609012604| Train Accuracy: 80.1809443030817% | Test Accuracy: 89.18996006845407%\n",
            "Epoch:[240/1000]| Loss: 0.41534557938575745| Train Accuracy: 80.5908962397512% | Test Accuracy: 89.18996006845407%\n",
            "Epoch:[241/1000]| Loss: 0.4148113429546356| Train Accuracy: 80.60503251342946% | Test Accuracy: 89.61779806046776%\n",
            "Epoch:[242/1000]| Loss: 0.415852814912796| Train Accuracy: 80.03958156629912% | Test Accuracy: 89.0188248716486%\n",
            "Epoch:[243/1000]| Loss: 0.4153362810611725| Train Accuracy: 80.46366977664687% | Test Accuracy: 89.50370792926412%\n",
            "Epoch:[244/1000]| Loss: 0.4153757393360138| Train Accuracy: 80.22335312411649% | Test Accuracy: 89.18996006845407%\n",
            "Epoch:[245/1000]| Loss: 0.4141295552253723| Train Accuracy: 80.6615776081425% | Test Accuracy: 89.36109526525956%\n",
            "Epoch:[246/1000]| Loss: 0.41421303153038025| Train Accuracy: 80.54848741871643% | Test Accuracy: 89.41814033086139%\n",
            "Epoch:[247/1000]| Loss: 0.41379305720329285| Train Accuracy: 80.52021487135991% | Test Accuracy: 89.04734740444951%\n",
            "Epoch:[248/1000]| Loss: 0.41438817977905273| Train Accuracy: 80.56262369239468% | Test Accuracy: 89.56075299486595%\n",
            "Epoch:[249/1000]| Loss: 0.4142460823059082| Train Accuracy: 80.23748939779473% | Test Accuracy: 89.10439247005134%\n",
            "Epoch:[250/1000]| Loss: 0.41479966044425964| Train Accuracy: 80.56262369239468% | Test Accuracy: 89.50370792926412%\n",
            "Epoch:[251/1000]| Loss: 0.41522541642189026| Train Accuracy: 80.12439920836867% | Test Accuracy: 88.8191671420422%\n",
            "Epoch:[252/1000]| Loss: 0.41539859771728516| Train Accuracy: 80.3223070398643% | Test Accuracy: 89.76041072447232%\n",
            "Epoch:[253/1000]| Loss: 0.415438711643219| Train Accuracy: 79.85581000848177% | Test Accuracy: 88.84768967484312%\n",
            "Epoch:[254/1000]| Loss: 0.4157314598560333| Train Accuracy: 80.56262369239468% | Test Accuracy: 89.76041072447232%\n",
            "Epoch:[255/1000]| Loss: 0.4169459939002991| Train Accuracy: 79.85581000848177% | Test Accuracy: 88.70507701083856%\n",
            "Epoch:[256/1000]| Loss: 0.416067898273468| Train Accuracy: 80.27989821882952% | Test Accuracy: 89.70336565887051%\n",
            "Epoch:[257/1000]| Loss: 0.4158341586589813| Train Accuracy: 79.82753746112525% | Test Accuracy: 88.93325727324587%\n",
            "Epoch:[258/1000]| Loss: 0.41498714685440063| Train Accuracy: 80.64744133446423% | Test Accuracy: 89.58927552766686%\n",
            "Epoch:[259/1000]| Loss: 0.4152528941631317| Train Accuracy: 80.27989821882952% | Test Accuracy: 88.90473474044495%\n",
            "Epoch:[260/1000]| Loss: 0.41427066922187805| Train Accuracy: 80.57675996607294% | Test Accuracy: 89.67484312606959%\n",
            "Epoch:[261/1000]| Loss: 0.41323795914649963| Train Accuracy: 80.26576194515125% | Test Accuracy: 89.38961779806047%\n",
            "Epoch:[262/1000]| Loss: 0.4131333827972412| Train Accuracy: 80.68985015549902% | Test Accuracy: 89.18996006845407%\n",
            "Epoch:[263/1000]| Loss: 0.4130176603794098| Train Accuracy: 80.73225897653379% | Test Accuracy: 89.2470051340559%\n",
            "Epoch:[264/1000]| Loss: 0.4129525125026703| Train Accuracy: 80.73225897653379% | Test Accuracy: 89.38961779806047%\n",
            "Epoch:[265/1000]| Loss: 0.41226357221603394| Train Accuracy: 80.71812270285554% | Test Accuracy: 89.50370792926412%\n",
            "Epoch:[266/1000]| Loss: 0.41260871291160583| Train Accuracy: 80.27989821882952% | Test Accuracy: 89.21848260125499%\n",
            "Epoch:[267/1000]| Loss: 0.4121989607810974| Train Accuracy: 80.78880407124683% | Test Accuracy: 89.36109526525956%\n",
            "Epoch:[268/1000]| Loss: 0.41285935044288635| Train Accuracy: 80.49194232400339% | Test Accuracy: 89.07586993725043%\n",
            "Epoch:[269/1000]| Loss: 0.41267499327659607| Train Accuracy: 80.6615776081425% | Test Accuracy: 89.67484312606959%\n",
            "Epoch:[270/1000]| Loss: 0.4159063398838043| Train Accuracy: 79.79926491376872% | Test Accuracy: 88.13462635482031%\n",
            "Epoch:[271/1000]| Loss: 0.42029446363449097| Train Accuracy: 80.02544529262087% | Test Accuracy: 88.8191671420422%\n",
            "Epoch:[272/1000]| Loss: 0.43871262669563293| Train Accuracy: 78.90867967203845% | Test Accuracy: 87.99201369081574%\n",
            "Epoch:[273/1000]| Loss: 0.4191656708717346| Train Accuracy: 79.99717274526435% | Test Accuracy: 89.58927552766686%\n",
            "Epoch:[274/1000]| Loss: 0.4131266176700592| Train Accuracy: 80.01130901894261% | Test Accuracy: 89.73188819167142%\n",
            "Epoch:[275/1000]| Loss: 0.4121815264225006| Train Accuracy: 80.44953350296862% | Test Accuracy: 88.70507701083856%\n",
            "Epoch:[276/1000]| Loss: 0.4156557023525238| Train Accuracy: 80.26576194515125% | Test Accuracy: 89.4751853964632%\n",
            "Epoch:[277/1000]| Loss: 0.4186820983886719| Train Accuracy: 79.45999434549053% | Test Accuracy: 89.16143753565316%\n",
            "Epoch:[278/1000]| Loss: 0.41444143652915955| Train Accuracy: 80.47780605032513% | Test Accuracy: 89.84597832287507%\n",
            "Epoch:[279/1000]| Loss: 0.4117382764816284| Train Accuracy: 80.44953350296862% | Test Accuracy: 89.73188819167142%\n",
            "Epoch:[280/1000]| Loss: 0.41138342022895813| Train Accuracy: 80.60503251342946% | Test Accuracy: 89.10439247005134%\n",
            "Epoch:[281/1000]| Loss: 0.4125981032848358| Train Accuracy: 80.61916878710772% | Test Accuracy: 89.53223046206503%\n",
            "Epoch:[282/1000]| Loss: 0.41623517870903015| Train Accuracy: 79.5024031665253% | Test Accuracy: 88.61950941243583%\n",
            "Epoch:[283/1000]| Loss: 0.41737842559814453| Train Accuracy: 80.3223070398643% | Test Accuracy: 89.33257273245864%\n",
            "Epoch:[284/1000]| Loss: 0.4285435676574707| Train Accuracy: 79.09245122985581% | Test Accuracy: 88.41985168282943%\n",
            "Epoch:[285/1000]| Loss: 0.42075660824775696| Train Accuracy: 80.16680802940344% | Test Accuracy: 89.41814033086139%\n",
            "Epoch:[286/1000]| Loss: 0.4227721691131592| Train Accuracy: 79.19140514560362% | Test Accuracy: 89.04734740444951%\n",
            "Epoch:[287/1000]| Loss: 0.4133967459201813| Train Accuracy: 80.44953350296862% | Test Accuracy: 89.27552766685682%\n",
            "Epoch:[288/1000]| Loss: 0.4116010069847107| Train Accuracy: 80.61916878710772% | Test Accuracy: 89.76041072447232%\n",
            "Epoch:[289/1000]| Loss: 0.41627755761146545| Train Accuracy: 79.82753746112525% | Test Accuracy: 88.84768967484312%\n",
            "Epoch:[290/1000]| Loss: 0.41641736030578613| Train Accuracy: 80.3505795872208% | Test Accuracy: 89.84597832287507%\n",
            "Epoch:[291/1000]| Loss: 0.41732287406921387| Train Accuracy: 79.92649137687306% | Test Accuracy: 89.2470051340559%\n",
            "Epoch:[292/1000]| Loss: 0.41254931688308716| Train Accuracy: 80.71812270285554% | Test Accuracy: 89.36109526525956%\n",
            "Epoch:[293/1000]| Loss: 0.41117456555366516| Train Accuracy: 80.71812270285554% | Test Accuracy: 89.64632059326868%\n",
            "Epoch:[294/1000]| Loss: 0.41530922055244446| Train Accuracy: 79.7427198190557% | Test Accuracy: 88.47689674843126%\n",
            "Epoch:[295/1000]| Loss: 0.41753900051116943| Train Accuracy: 80.40712468193384% | Test Accuracy: 89.07586993725043%\n",
            "Epoch:[296/1000]| Loss: 0.427106648683548| Train Accuracy: 79.05004240882103% | Test Accuracy: 88.64803194523674%\n",
            "Epoch:[297/1000]| Loss: 0.41611939668655396| Train Accuracy: 80.44953350296862% | Test Accuracy: 89.93154592127782%\n",
            "Epoch:[298/1000]| Loss: 0.4127890169620514| Train Accuracy: 79.96890019790783% | Test Accuracy: 89.4751853964632%\n",
            "Epoch:[299/1000]| Loss: 0.41102996468544006| Train Accuracy: 80.68985015549902% | Test Accuracy: 89.41814033086139%\n",
            "Epoch:[300/1000]| Loss: 0.41016286611557007| Train Accuracy: 80.73225897653379% | Test Accuracy: 89.36109526525956%\n",
            "Epoch:[301/1000]| Loss: 0.41089919209480286| Train Accuracy: 80.78880407124683% | Test Accuracy: 89.58927552766686%\n",
            "Epoch:[302/1000]| Loss: 0.4095765948295593| Train Accuracy: 80.73225897653379% | Test Accuracy: 89.81745579007416%\n",
            "Epoch:[303/1000]| Loss: 0.41105881333351135| Train Accuracy: 80.23748939779473% | Test Accuracy: 89.33257273245864%\n",
            "Epoch:[304/1000]| Loss: 0.4099913239479065| Train Accuracy: 80.73225897653379% | Test Accuracy: 89.84597832287507%\n",
            "Epoch:[305/1000]| Loss: 0.41145244240760803| Train Accuracy: 80.42126095561211% | Test Accuracy: 88.8191671420422%\n",
            "Epoch:[306/1000]| Loss: 0.4120899438858032| Train Accuracy: 80.43539722929036% | Test Accuracy: 89.21848260125499%\n",
            "Epoch:[307/1000]| Loss: 0.4198693633079529| Train Accuracy: 79.27622278767316% | Test Accuracy: 88.19167142042214%\n",
            "Epoch:[308/1000]| Loss: 0.4233461916446686| Train Accuracy: 79.78512864009048% | Test Accuracy: 88.73359954363947%\n",
            "Epoch:[309/1000]| Loss: 0.4473840594291687| Train Accuracy: 78.62595419847328% | Test Accuracy: 89.30405019965772%\n",
            "Epoch:[310/1000]| Loss: 0.41157960891723633| Train Accuracy: 80.63330506078597% | Test Accuracy: 86.9937250427838%\n",
            "Epoch:[311/1000]| Loss: 0.437572717666626| Train Accuracy: 78.85213457732542% | Test Accuracy: 87.82087849401027%\n",
            "Epoch:[312/1000]| Loss: 0.46429553627967834| Train Accuracy: 77.74950523042125% | Test Accuracy: 88.50541928123218%\n",
            "Epoch:[313/1000]| Loss: 0.45572787523269653| Train Accuracy: 78.59768165111677% | Test Accuracy: 85.39646320593268%\n",
            "Epoch:[314/1000]| Loss: 0.4609517753124237| Train Accuracy: 76.81651116765622% | Test Accuracy: 89.53223046206503%\n",
            "Epoch:[315/1000]| Loss: 0.4709007143974304| Train Accuracy: 80.39298840825559% | Test Accuracy: 86.82258984597833%\n",
            "Epoch:[316/1000]| Loss: 0.46572285890579224| Train Accuracy: 77.18405428329092% | Test Accuracy: 87.10781517398745%\n",
            "Epoch:[317/1000]| Loss: 0.4584082067012787| Train Accuracy: 77.4950523042126% | Test Accuracy: 89.13291500285226%\n",
            "Epoch:[318/1000]| Loss: 0.4414661228656769| Train Accuracy: 80.02544529262087% | Test Accuracy: 85.51055333713634%\n",
            "Epoch:[319/1000]| Loss: 0.47288209199905396| Train Accuracy: 77.67882386202997% | Test Accuracy: 89.98859098687963%\n",
            "Epoch:[320/1000]| Loss: 0.43816545605659485| Train Accuracy: 79.33276788238621% | Test Accuracy: 88.30576155162578%\n",
            "Epoch:[321/1000]| Loss: 0.4555922746658325| Train Accuracy: 77.86259541984732% | Test Accuracy: 88.8191671420422%\n",
            "Epoch:[322/1000]| Loss: 0.45030277967453003| Train Accuracy: 77.9756856092734% | Test Accuracy: 89.78893325727326%\n",
            "Epoch:[323/1000]| Loss: 0.4352220594882965| Train Accuracy: 80.27989821882952% | Test Accuracy: 86.28066172276098%\n",
            "Epoch:[324/1000]| Loss: 0.4538577198982239| Train Accuracy: 78.37150127226464% | Test Accuracy: 89.27552766685682%\n",
            "Epoch:[325/1000]| Loss: 0.43067696690559387| Train Accuracy: 78.86627085100368% | Test Accuracy: 89.07586993725043%\n",
            "Epoch:[326/1000]| Loss: 0.44404345750808716| Train Accuracy: 78.37150127226464% | Test Accuracy: 89.50370792926412%\n",
            "Epoch:[327/1000]| Loss: 0.4230184257030487| Train Accuracy: 79.81340118744698% | Test Accuracy: 85.59612093553908%\n",
            "Epoch:[328/1000]| Loss: 0.4553716480731964| Train Accuracy: 77.53746112524739% | Test Accuracy: 89.04734740444951%\n",
            "Epoch:[329/1000]| Loss: 0.44735127687454224| Train Accuracy: 78.47045518801245% | Test Accuracy: 89.07586993725043%\n",
            "Epoch:[330/1000]| Loss: 0.44669294357299805| Train Accuracy: 78.4845914616907% | Test Accuracy: 87.56417569880205%\n",
            "Epoch:[331/1000]| Loss: 0.43784618377685547| Train Accuracy: 79.41758552445576% | Test Accuracy: 88.27723901882487%\n",
            "Epoch:[332/1000]| Loss: 0.4314649999141693| Train Accuracy: 79.92649137687306% | Test Accuracy: 88.8191671420422%\n",
            "Epoch:[333/1000]| Loss: 0.43596404790878296| Train Accuracy: 78.45631891433418% | Test Accuracy: 88.67655447803764%\n",
            "Epoch:[334/1000]| Loss: 0.43515974283218384| Train Accuracy: 78.35736499858638% | Test Accuracy: 88.73359954363947%\n",
            "Epoch:[335/1000]| Loss: 0.4283183813095093| Train Accuracy: 80.3505795872208% | Test Accuracy: 87.56417569880205%\n",
            "Epoch:[336/1000]| Loss: 0.43372246623039246| Train Accuracy: 79.71444727169919% | Test Accuracy: 89.16143753565316%\n",
            "Epoch:[337/1000]| Loss: 0.42621713876724243| Train Accuracy: 79.03590613514278% | Test Accuracy: 88.22019395322305%\n",
            "Epoch:[338/1000]| Loss: 0.43256595730781555| Train Accuracy: 78.54113655640373% | Test Accuracy: 89.30405019965772%\n",
            "Epoch:[339/1000]| Loss: 0.42096227407455444| Train Accuracy: 80.27989821882952% | Test Accuracy: 87.84940102681118%\n",
            "Epoch:[340/1000]| Loss: 0.43144160509109497| Train Accuracy: 79.98303647158609% | Test Accuracy: 89.87450085567599%\n",
            "Epoch:[341/1000]| Loss: 0.4194687008857727| Train Accuracy: 79.68617472434266% | Test Accuracy: 89.07586993725043%\n",
            "Epoch:[342/1000]| Loss: 0.4272315204143524| Train Accuracy: 78.93695221939497% | Test Accuracy: 89.61779806046776%\n",
            "Epoch:[343/1000]| Loss: 0.4176481366157532| Train Accuracy: 80.6615776081425% | Test Accuracy: 88.41985168282943%\n",
            "Epoch:[344/1000]| Loss: 0.42336082458496094| Train Accuracy: 80.25162567147301% | Test Accuracy: 89.38961779806047%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc"
      ],
      "metadata": {
        "id": "zb1BKZhhy2_s",
        "outputId": "834e3038-e3c0-4026-b234-70152deee359",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90.13120365088419"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFgGo4hGtUlC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39lWMvpwux0H"
      },
      "execution_count": 56,
      "outputs": []
    }
  ]
}